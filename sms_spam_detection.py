# -*- coding: utf-8 -*-
"""sms spam detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kpSCPEL5rHscpUc0_3eJR3-tDNihsvj-
"""

import numpy as np
import pandas as pd

df = pd.read_csv('/content/sms spam dataset.csv',encoding='ISO-8859-1')

df.head()

df.shape

"""# 1.Data cleaning"""

df.info()

df.head()

df = df.rename(columns={'v1':'Target','v2':'Text'})

df.head()

df = df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'])

df.head()

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['Target'] = encoder.fit_transform(df['Target'])

df.head()

df.isnull().sum()

df.duplicated().sum()

df = df.drop_duplicates(keep='first')

df.duplicated().sum()

df.shape

df = df[['Text', 'Target']]

df.head()

"""## 2.Exploratory Data Analysis"""

df['Target'].value_counts()

import matplotlib.pyplot as plt
plt.pie(df['Target'].value_counts(), labels=['not_spam','spam'],autopct="%0.2f")
plt.show()

!pip install nltk

import nltk

nltk.download('punkt')

df['num_characters'] = df['Text'].apply(len)
df['num_words'] = df['Text'].apply(lambda x:len(nltk.word_tokenize(x)))
df['num_sentences'] = df['Text'].apply(lambda x:len(nltk.sent_tokenize(x)))
df.head()

import seaborn as sns

df[df['Target'] == 0][['num_characters','num_words','num_sentences']].describe()

df[df['Target'] == 1][['num_characters','num_words','num_sentences']].describe()

plt.figure(figsize=(12,6))
sns.histplot(df[df['Target'] == 0]['num_characters'])
sns.histplot(df[df['Target'] == 1]['num_characters'],color='red')

# red -> spam



plt.figure(figsize=(12,6))
sns.histplot(df[df['Target'] == 0]['num_words'])
sns.histplot(df[df['Target'] == 1]['num_words'],color='red')

plt.figure(figsize=(5,5))
sns.heatmap(df.corr(), cbar=True, square=True, fmt='.3f', annot=True, annot_kws={'size':8}, cmap='Blues')

"""# 3. Data Preprocessing"""

nltk.download('stopwords')

from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
ps = PorterStemmer()
stopwords.words('english')
import string

def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)

    y = []
    for i in text:
        if i.isalnum() and i not in stopwords.words('english') and i not in string.punctuation:
            y.append(ps.stem(i))

    return " ".join(y)

df['transformed_text'] = df['Text'].apply(transform_text)
df.head()

from wordcloud import WordCloud

wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')
spam_wc = wc.generate(df[df['Target'] == 1]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(15,6))
plt.imshow(spam_wc)

spam_corpus = []
for msg in df[df['Target'] == 1]['transformed_text'].tolist():
    for word in msg.split():
        spam_corpus.append(word)

len(spam_corpus)

from collections import Counter
word_counts = Counter(spam_corpus)

most_common_words = word_counts.most_common(30)

df_most_common_words = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])

sns.barplot(x='Word', y='Frequency', data=df_most_common_words)
plt.xticks(rotation='vertical')
plt.show()

"""# 4. Model Building"""

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv = CountVectorizer()
tfidf = TfidfVectorizer(max_features=3000)

X = tfidf.fit_transform(df['transformed_text']).toarray()

#from sklearn.preprocessing import MinMaxScaler
#scaler = MinMaxScaler()
#X = scaler.fit_transform(X)

# appending the num_character col to X
#X = np.hstack((X,df['num_characters'].values.reshape(-1,1)))

X.shape

y = df['Target'].values

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

gnb.fit(X_train,y_train)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(X_train,y_train)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))

bnb.fit(X_train,y_train)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))

"""tfidf --> MNB"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

svc = SVC(kernel='sigmoid', gamma=1.0)
knc = KNeighborsClassifier()
mnb = MultinomialNB()
dtc = DecisionTreeClassifier(max_depth=5)
lrc = LogisticRegression(solver='liblinear', penalty='l1')
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
abc = AdaBoostClassifier(n_estimators=50, random_state=2)
bc = BaggingClassifier(n_estimators=50, random_state=2)
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)
xgb = XGBClassifier(n_estimators=50,random_state=2)

clfs = {
    'SVC' : svc,
    'KN' : knc,
    'NB': mnb,
    'DT': dtc,
    'LR': lrc,
    'RF': rfc,
    'AdaBoost': abc,
    'BgC': bc,
    'ETC': etc,
    'GBDT':gbdt,
    'xgb':xgb
}

def train_classifier(clf,X_train,y_train,X_test,y_test):
    clf.fit(X_train,y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test,y_pred)
    precision = precision_score(y_test,y_pred)

    return accuracy,precision
train_classifier(svc,X_train,y_train,X_test,y_test)

accuracy_scores = []
precision_scores = []

for name,clf in clfs.items():

    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)

    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)

    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)

performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)

performance_df

performance_df['Mean'] = performance_df[['Accuracy', 'Precision']].mean(axis=1)

performance_df

df_sorted = performance_df.sort_values('Mean', ascending=False)

# Set the width of each bar
bar_width = 0.35

# Create a numpy array for x-axis positions
x_pos = np.arange(len(df_sorted))

# Plot the bar graph
plt.figure(figsize=(12, 6))
plt.bar(x_pos, df_sorted['Accuracy'], width=bar_width, label='Accuracy', alpha=0.7)
plt.bar(x_pos + bar_width, df_sorted['Precision'], width=bar_width, label='Precision', alpha=0.7)
plt.xlabel('Algorithm')
plt.ylabel('Score')
plt.xticks(x_pos + bar_width / 2, df_sorted['Algorithm'], rotation='vertical')
plt.legend()
plt.ylim(0.8, 1.0)  # Set the y-axis limit to better visualize the differences
plt.title('Accuracy and Precision for each Algorithm (Descending Order)')
plt.tight_layout()  # Ensure that the labels and ticks fit within the figure
plt.show()

import pickle
pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(mnb,open('model.pkl','wb'))